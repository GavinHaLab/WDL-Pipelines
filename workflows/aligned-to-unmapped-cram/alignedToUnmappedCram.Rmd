---
title: "Aligned bam to Unmapped bam"
author: "Amy Paguirigan"
editor_options: 
  chunk_output_type: console
---
# Install Required Packages

## Installpakcages
```{r}
remotes::install_github('FredHutch/tgR.data')
remotes::install_github('FredHutch/fh.wdlR')
```

## Load Packages
```{r}
library(jsonlite); library(tidyverse); library(tgR.data); library(aws.s3); library(fh.wdlR); 
```

# Set Credentials
```{r}
set_credentials(path = "~/github/cred/indexingCreds.R")
```

# Pull S3 Inventory and Annotate
```{r}
S3bucket <- "fh-pi-ha-g-eco"
bams <- "bamCramSam"
prefix <- "Collaborator_Data/Broad_Terra/CRPC_patients-pdx_WGS/Gavin_Ha_Fred_Hutch_Prostate_Cancer_WGS_data/RP-2253"

bamFiles <- get_file_metadata(bucket = S3bucket, file_type = bams, prefix = prefix)

selectFiles <- bamFiles %>% filter(dir_level_1 == "PDX_mouseSubtraction" & grepl("recalibrated.bam", object_key))

dataProv <- get_data_provenance(bucket = selectFiles$bucket_name, 
                                prefix = selectFiles$bucket_prefix, 
                                uuid = selectFiles$uuid, allowEmpty = F)

monsterMash <- left_join(selectFiles, dataProv)

monsterMash$dataset_id <- stringr::str_match(monsterMash$object_key, ".*/([^/]*)/[^/]*")[,2]
```


# Filter Inventory for Workflow Input Data
In this workflow, the input data must be bam files and their dataset ids and sample names. 

```{r}
# rename("sample_name" = "omics_sample_name")
manifest <- monsterMash %>% 
  mutate(sample_name = paste("test", seq(1:nrow(monsterMash)), sep = "-"), alignedBam = paste("s3:/", bucket_name,  object_key, sep = "/")) %>% 
  select(sample_name, dataset_id, alignedBam)

manifest <- manifest[1,]
batchFileName <- paste0(format(Sys.Date(), "%Y-%m-%d-"), "batchOfOne.json")
jsonlite::write_json(manifest, batchFileName, auto_unbox = TRUE)

```

# Edit and Store Batch File, WDL and/or Parameters File
The framework around the batch file may need additional information provided manually.  After editing the basic framework provided by the above process and checking to make sure that your copy of the WDL and any parameters file is final, then copy the final workflow description to S3 for future reference.

```{r}
list.files()
paramsFile <- ""
wdlFile <- "alignedToUnmappedCram.wdl"
  
put_object(batchFileName,
          object = batchFileName,
          bucket = paste0("fh-pi-paguirigan-a-eco/cromwell-files/Ha/alignedToUnmappedCram/",format(Sys.Date(), "%Y-%m-%d")),
          acl = "bucket-owner-full-control")

put_object(wdlFile,
          object = wdlFile,
          bucket = paste0("fh-pi-paguirigan-a-eco/cromwell-files/Ha/alignedToUnmappedCram/",format(Sys.Date(), "%Y-%m-%d")),
          acl = "bucket-owner-full-control")



```

# Submit Job to Cromwell
Note: setwd to location of workflow files
```{r}
setCromwellURL("gizmok87:46459")
cromwellValidate(WDL = wdlFile, allInputs = batchFileName)

thisJob <- cromwellSubmitBatch(WDL = wdlFile,
                    Batch = batchFileName,
                    Options = "../../workflow-options/noNewCalls-callcachingOn.json",
                    Labels = data.frame("workflowType" = "no cache tastk"))
thisOne <- thisJob$id; thisOne
```

# Monitor Running Jobs
You can monitor running jobs on a Cromwell server by going to: https://cromwellapp.fredhutch.org/  OR you can do it here.
```{r}
w <- cromwellWorkflow(thisOne)
c <- cromwellCall(thisOne); c %>% group_by(executionStatus, callName) %>% summarize(status = n())
ca <- cromwellCache(thisOne); ca %>% group_by(callCaching.hit, callName) %>% summarize(hits = n())
butWhy <- left_join(cromwellCall(thisOne), mutate_all(cromwellCache(thisOne), as.character)); butWhy %>% 
    group_by(callName, executionStatus, callCaching.hit) %>% summarize(hits = n()) %>% 
    arrange(desc(executionStatus))
f <- cromwellFailures(thisOne)
#abort <- cromwellAbort(thisOne) # Careful with this
WTF <- cromwellGlob(thisOne)


```



# Output Processing
```{r}
out <- cromwellOutputs(thisOne)
```

