---
title: "Aligned bam to Unmapped bam"
author: "Amy Paguirigan"
date: "11/12/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---
# Install Required Packages

## For Local Use
```{r}
devtools::install_github('FredHutch/tgR')
devtools::install_github("dtenenba/aws.s3", ref="feature/add-tagging-functions")
```

## For Rhino Use
```{r}
devtools::install_github('FredHutch/tgR', lib = "~/R/x86_64-pc-linux-gnu-library/3.5") # Rhino
devtools::install_github("dtenenba/aws.s3", ref="feature/add-tagging-functions", lib = "~/R/x86_64-pc-linux-gnu-library/3.5")
```

# Load Packages
```{r}
library(tidyverse); library(aws.s3); library(tgR); library(furrr)
```

# Set Credentials (run one line)
```{r}
setCreds(tokenSet = "file", path = "~/Documents/GitHubRepos/paguirigan.R") #DVTC
setCreds(tokenSet = "file", path = "~/Documents/github/paguirigan.R") #laptop
setCreds(tokenSet = "file", path = "~/cred/paguirigan.R") #rhino
```

# Pull S3 Inventory and Annotate
```{r}
tags <- listS3Objects(bucket = "fh-pi-paguirigan-a")
annotations <- tgrAnnotate(DAG = "paguirigana", harmonizedOnly = TRUE)
monsterMash <- dplyr::left_join(tags, annotations)
```

# Pull S3 Inventory and Annotate  - Alice Berger
```{r}
tags <- listS3Objects(bucket = "fh-pi-berger-a")
annotations <- tgrAnnotate(DAG = "bergera", harmonizedOnly = TRUE)
monsterMash <- dplyr::left_join(tags, annotations)
```


# Filter Inventory for Workflow Input Data
In this workflow, the input data must be paired fastq's with the pattern "_R1_" and "_R2_" in the file names

```{r}
studyData <- monsterMash %>% 
    filter(stage == "processed" & grepl("*.bam$", key)==T)
studyData$prefix <- paste0("s3://", studyData$pi_bucket,"/",studyData$key)



manifest <- studyData %>% 
    select(molecular_id, omics_sample_name, prefix) %>% rename("alignedBamURL" = "prefix")
manifest[manifest$molecular_id == "referenceDataSet",]$omics_sample_name <- "CEPH"
```


# Prefilter manifest if needed
```{r}
manifest <- manifest[1,]
```


# Name and Ship Batch File
```{r}
batchFileName <- paste0("cromwell-manifests/",format(Sys.Date(), "%Y-%m-%d-"), "localalignedToUnmappedTEst.tsv")
paste0("s3://fh-pi-paguirigan-a-genomicsrepo/", batchFileName)
s3write_using(manifest,
              FUN = write.table, quote = F, row.names = F, sep = "\t",
              object = batchFileName,
              bucket = "fh-pi-paguirigan-a-genomicsrepo")
thisManifest <- "cromwell-manifests/2019-11-12-localalignedToUnmappedTEst.tsv" # for new environments
```

# Submit Job to Cromwell
Note: setwd to location of workflow files
```{r}
thisJob <- cromwellSubmitBatch(WDL = "Workflows/zoanthropy.wdl",
                    Batch = "Workflows/zoanthropy-batch.json",
                    Options = "Workflows/workflow-options.json",
                    Labels = data.frame("workflowType" = "firstRun"))
thisOne <- thisJob$id; thisOne
```

# Monitor Running Jobs
```{r}
w <- cromwellWorkflow(thisOne)
c <- cromwellCall(thisOne); c %>% group_by(executionStatus, callName) %>% summarize(status = n())
ca <- cromwellCache(thisOne); ca %>% group_by(callCaching.hit, callName) %>% summarize(hits = n())
butWhy <- left_join(cromwellCall(thisOne), mutate_all(cromwellCache(thisOne), as.character)); butWhy %>% 
    group_by(callName, executionStatus, callCaching.hit) %>% summarize(hits = n()) %>% 
    arrange(desc(executionStatus))
f <- cromwellFailures(thisOne)
#abort <- cromwellAbort(thisOne) # Careful with this
WTF <- cromwellGlob(thisOne)

c %>% filter(callName == "PairedFastqsToUnmappedBam.ValidateBam") %>% group_by(executionStatus) %>% summarize(status = n())
```


# Current or Previous Runs to Monitor
```{r}
thisOne <- "4aa35895-b457-4fd1-bf41-e75953881071" #first test

```


# Output Processing
```{r}
out <- cromwellOutputs(thisOne)

batchFile <- s3read_using(FUN = read.delim,
                           object = thisManifest,
                           bucket = "fh-pi-paguirigan-a-genomicsrepo")
batchFile$shardIndex <- as.character(seq(from = 0, to = nrow(batchFile)-1, by = 1))
annotatedOutputs <- inner_join(batchFile, out, by = c("shardIndex")) # if no molecular_id


copyNTag <- annotatedOutputs %>%
  select(molecular_id, workflowOutputType, s3URL, s3Prefix, workflow_id, workflowName, s3Bucket) %>%
  rename("workflowID" = "workflow_id")
copyNTag$s3DestinationPrefix <- gsub( "cromwell-output/", "tg/LILAC-FARBER/TGR-Analyses/",gsub("call-RevertThisBam/shard-[^/]*/", "", copyNTag$s3Prefix))
copyNTag$s3DestinationBucket <- "fh-pi-berger-a"
copyNTag$stage <- "raw"
copyNTag[10,]
```


# Copy Outputs and Tag
```{r}
colnames(copyNTag)
#Only keep columns you need or want to use as tags
readyToRoll <- copyNTag %>% select(s3Prefix, s3Bucket, s3DestinationPrefix, s3DestinationBucket, molecular_id, stage, workflowID, workflowName); colnames(readyToRoll)

QuitePossibly <- prep_s3_copy_and_tag(readyToRoll); QuitePossibly[1]

future::plan(strategy = multiprocess)
furrr::future_map(QuitePossibly, function(x) {
  s3_copy_and_tag(fromBucket = x$s3Bucket,
                  fromPrefix = x$s3Prefix,
                  toBucket = x$s3DestinationBucket,
                  toPrefix = x$s3DestinationPrefix,
                  tagList = x$tagSet)
  print(x$s3DestinationPrefix)
})
```

# For Large file copies you have to use the command line
Change WD to directory on Fast where the output-copy-and-tag.sh script (and arrayjob.sh) are saved for this project.  
```{r}
write.table(readyToRoll, file = "2019-06-18-zoanthropy-LILACtest-copy.txt",
            sep = " ", na = "", row.names = F, col.names = F, quote = F)
```


```{bash}
ssh apaguiri@rhino
module load awscli
sbatch --job-name 2019-06-18-zoanthropy-LILACtest-copy.txt arrayjob.sh
bash monitorjobs.sh <jobID>
```
## FINAL NOTE:  copy the WDL used and any parameter files to S3 into the final data directory too!!!!
